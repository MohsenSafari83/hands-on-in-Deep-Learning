# ðŸš€ Optimization in Neural Networks

## Introduction
The foundation of neural network training lies in **Gradient Descent**.  
However, most neural networks are based on **non-convex loss functions** with **highly complex loss surfaces** â€” containing multiple local minima, saddle points, and flat regions.  

As a result, **vanilla Gradient Descent** often struggles to find the true global minimum:
- It may get trapped in **local minima**.  
- It can **oscillate** in narrow valleys.  
- It depends heavily on a properly chosen **learning rate**.  

To overcome these limitations, advanced optimizers such as **Momentum**, **Adam**, and **Newtonâ€™s Method** were developed.

---

## 1ï¸âƒ£ Gradient Descent â€” Overview

**Goal:** Minimize the loss function `J(w)` with respect to weights `w`:

`w* = argmin_w J(w)`

**Update Rule:**

`w_{t+1} = w_t - Î· âˆ‡_w J(w_t)`

**Limitations:**
- Sensitive to the choice of **learning rate** `Î·`.  
- Can get **stuck** in local minima.  
- **Slow convergence** in flat regions.  
- **Oscillations** in steep or irregular loss surfaces.

---

## 2ï¸âƒ£ Momentum Optimization

Momentum introduces **memory** into gradient descent â€” it accumulates the direction of past gradients to build â€œvelocityâ€ that helps escape shallow minima and smooths oscillations.

### First Momentum (Classic Momentum)
**Idea:** Maintain an exponentially decaying moving average of past gradients.

**Update Equations:**

`m_{t+1} = Î²â‚ m_t + (1 - Î²â‚) âˆ‡_w J(w_t)`

`w_{t+1} = w_t - Î· m_{t+1}`

where `Î²â‚ âˆˆ [0.9, 0.99]` controls the influence of past gradients.

---

### Second Momentum (Variance Term)
Tracks the **moving average of squared gradients**, controlling step size adaptively:

`v_{t+1} = Î²â‚‚ v_t + (1 - Î²â‚‚) (âˆ‡_w J(w_t))Â²`

---

### Bias Correction
To counter early-step bias (since `mâ‚€ = vâ‚€ = 0`):

`mÌ‚_t = m_t / (1 - Î²â‚^t)`  
`vÌ‚_t = v_t / (1 - Î²â‚‚^t)`

---

### Summary:
- **First Momentum â†’** direction (velocity)  
- **Second Momentum â†’** step size (adaptive scaling)  
- Combined, they form the basis for the **Adam optimizer**.

---

## 3ï¸âƒ£ Adam Optimizer (Adaptive Moment Estimation)

Adam combines both **momentum** and **adaptive learning rate** mechanisms.

**Equations:**

`m_{t+1} = Î²â‚ m_t + (1 - Î²â‚) âˆ‡_w J(w_t)`  
`v_{t+1} = Î²â‚‚ v_t + (1 - Î²â‚‚) (âˆ‡_w J(w_t))Â²`

`mÌ‚_{t+1} = m_{t+1} / (1 - Î²â‚^{t+1})`  
`vÌ‚_{t+1} = v_{t+1} / (1 - Î²â‚‚^{t+1})`

`w_{t+1} = w_t - Î· * (mÌ‚_{t+1} / (âˆš(vÌ‚_{t+1}) + Îµ))`

**Typical Parameters:**  
`Î²â‚ = 0.9`, `Î²â‚‚ = 0.999`, `Îµ = 10â»â¸`

**Advantages:**
- Fast convergence  
- Works well with sparse or noisy gradients  
- Minimal hyperparameter tuning required  

---

## 4ï¸âƒ£ Newtonâ€™s Method

Newtonâ€™s Method uses **second-order derivatives** to adjust the update direction and step size using curvature information (the Hessian matrix).

**Concept:**
Find `x` where `f'(x) = 0`, then iteratively update:

`x_{t+1} = x_t - f'(x_t) / f''(x_t)`

For optimization in multiple dimensions:

`w_{t+1} = w_t - Hâ»Â¹ âˆ‡_w J(w_t)`

where `H` is the **Hessian matrix** (matrix of second derivatives).

**Pros:**
- Quadratic convergence near minima  
- Adaptive step sizes via curvature  

**Cons:**
- Requires Hessian computation (expensive for large models)  
- Can converge to saddle points in non-convex surfaces  

---

## Summary

| Optimizer | Type | Memory | Adaptivity | Pros | Cons |
|------------|------|---------|-------------|------|------|
| **Gradient Descent** | First-order | Low | âœ— | Simple, intuitive | Sensitive to LR, slow |
| **Momentum** | First-order + history | Medium | âœ— | Smoother, faster convergence | Needs tuning (Î²â‚) |
| **Adam** | First-order adaptive | Medium | âœ“ | Fast, robust, adaptive | Sometimes poorer generalization |
| **Newtonâ€™s Method** | Second-order | High | âœ“ | Fast near minima | Computationally heavy |

---

> âš¡ **In short:**  
> Neural network optimization starts with Gradient Descent, improves with Momentum, adapts with Adam, and theoretically perfects with Newtonâ€™s Method â€” though practical constraints often decide which is used.
